# DigitalHuman-survey
我们调研了数字人相关的论文、项目以及国内外现状等，并对各方面调研、资料进行了整合和归纳。

---



# Speak From Heart: 基于情感引导的LLM多模态对话生成方法

![image](https://github.com/user-attachments/assets/4582a223-9e19-4a0b-874c-b5732a4d2a9f)

## 简介
本论文提出了 **ELMD**（Emotion-Guided LLM-Based Multimodal Dialogue），一种基于大语言模型（LLM）的情感引导多模态对话框架。ELMD通过融合文本和视觉线索，显著提升了对话系统的情感表达能力，能够生成更具情感共鸣且贴近人类的自然对话内容，解决了传统情感对话生成中的诸多局限性。

## 方法
ELMD框架主要由以下三个核心模块组成：
1. **情感检索模块（ERM）：** 使用对比学习获取细粒度的情感表示，为生成示例提供语义和情感相关内容。
2. **情感预测模块（REP）：** 将视觉信息中的情感线索与生成的响应关联起来，增强情感理解能力。
3. **情感增强响应生成模块（EERG）：** 融合多模态输入，生成流畅且富有情感的对话内容。

通过两阶段训练策略，ELMD能够捕捉细腻情感并实现多模态信息的高效协同。

## 实验结果
在两个真实场景的中文多模态数据集（M3ED 和 MMED）上进行的实验表明，ELMD在 BLEU、METEOR、ROUGE 等评价指标上显著优于现有基线模型（如 VisualGLM 和 DialoGPT），模型展现了在情感表达和语境理解方面的强大性能。

---

# Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis 
![image](https://github.com/user-attachments/assets/9c8900e9-c788-47ef-98cb-eb8bc085d2d2)

## 简介
多模态情感分析（Multimodal Sentiment Analysis, MSA）整合了文本、音频和视觉等模态数据，用于分析情感。然而，由于数据标注的限制，现有方法难以有效平衡模态间的一致性和差异性。因此，本文提出了一个**Self-MM**框架，通过自监督多任务学习获取特定模态的表示：
- 利用**自监督标签生成模块**自动生成单模态标签，无需额外的人工标注。
- 通过**动态权重调整策略**平衡模态一致性和特定差异性。
- 在MOSI、MOSEI和SIMS等基准数据集上优于当前最先进方法。

## 方法
**Self-MM框架**主要包括以下模块：
1. **多模态任务**：通过预训练的BERT和单向LSTM模型处理文本、音频和视觉数据，生成融合的多模态表示。
2. **单模态子任务**：通过自动生成的标签训练单模态任务，捕捉特定模态的特征。
3. **单模态标签生成模块（ULGM）**：根据多模态数据动态计算标签，并通过动量更新策略在训练中保持稳定性。
4. **优化策略**：通过多任务损失函数结合权重调整机制，优先处理模态标签差异较大的样本。

## 实验结果
在三个广泛使用的多模态情感分析数据集上验证了Self-MM的性能：
- **MOSI**：在多模态情感回归任务中实现了最先进的相关性和分类准确率。
- **MOSEI**：在大规模数据集上展现了出色的鲁棒性，超过了MAG-BERT和MISA等基线模型。
- **SIMS**：在情感预测上与人工标注的标签表现相当，验证了自动生成标签的可靠性。
完整的实现代码和预处理数据集已开放，点击下方链接访问：
[**GitHub 仓库**](https://github.com/thuiar/Self-MM)

### 主要实验结果
| 数据集 | MAE ↓ | 相关性（Corr）↑ | 准确率（Acc-2）↑ | F1分数 ↑ |
|--------|--------|----------------|-----------------|----------|
| MOSI   | 0.713  | 0.798          | 84.00           | 85.98    |
| MOSEI  | 0.530  | 0.765          | 82.81           | 85.17    |
| SIMS   | 0.419  | 0.616          | 80.74           | 80.78    |

Self-MM显著提高了模态特定和模态一致表示的学习效果，自动生成的单模态标签为人工标注提供了一种经济高效的替代方案。

---

# Concept-Oriented Transformers for Visual Sentiment Analysis

## 简介
在当今的多媒体网络中，图像是表达情感的重要媒介，视觉情感分析旨在识别图像中的正面、负面或中性情感信号。这项研究提出了一种新的方法——**SentiViT**（概念导向的Transformer），将概念导向机制引入Transformer模型，用于视觉情感分析。主要贡献包括：
- 提出一种**概念导向的视觉情感分析框架**，可根据不同概念对同一图像进行不同的情感分类。
- 引入**概念导向的自注意力机制**，包括特征融合和注意力融合两种实现方式。
- 在多个真实数据集（如VSO和Yelp）上显著优于现有方法。


## 方法
基于Vision Transformer (ViT) 的图像处理框架，将图像分割为固定大小的补丁，通过自注意力机制提取特征。**概念导向机制**通过以下两种方式将概念信息融入模型：
   - **特征融合 (SentiViT-F)**：使用概念向量调整输入特征，提升与概念相关的特征。
   - **注意力融合 (SentiViT-A)**：在自注意力计算中引入基于特定概念的查询、键和值变换，增强对概念的建模能力。
   - 结合批量梯度下降和概念抽样策略，保证不同概念的样本均匀覆盖。


## 实验结果
在VSO和Yelp数据集上的实验表明，SentiViT模型在视觉情感分类任务上表现出色，并提供了更细粒度的概念建模能力。

- **VSO**数据集：包含从Flickr获取的带有情感标注的图像，以名词（如flower, lake）为概念。
- **Yelp**数据集：从在线评论中提取的图像，概念包括用户、商家和类别（如餐厅、宠物服务等）。

### 主要实验结果
| 模型         | 数据集   | 准确率（Accuracy）↑ | AUC（曲线下面积）↑ |
|--------------|----------|--------------------|--------------------|
| ResNet-152   | VSO      | 61.4              | 64.2              |
| EfficientNet-B7 | VSO  | 63.2              | 66.3              |
| ViT          | VSO      | 64.9              | 67.1              |
| SentiViT-F   | VSO      | 70.3              | 79.9              |
| SentiViT-A   | VSO      | **70.5**          | **80.4**          |


- **SentiViT-A**在所有数据集上的表现均优于基线模型和现有方法。
- 注意力融合（SentiViT-A）比特征融合（SentiViT-F）更具表达力，适用于更多场景。

模型中的自注意力组件对于情感预测具有重要影响，未来可以进一步探索其行为特性，以设计更高效的模型架构，并扩展视觉情感分析的应用场景。

---

