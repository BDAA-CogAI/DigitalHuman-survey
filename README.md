# DigitalHuman-survey
我们调研了数字人相关的论文、项目、国内外现状等，并对各方面调研、资料进行了整合和归纳。

# Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis 

## 简介
多模态情感分析（Multimodal Sentiment Analysis, MSA）整合了文本、音频和视觉等模态数据，用于分析情感。然而，由于数据标注的限制，现有方法难以有效平衡模态间的一致性和差异性。因此，本文提出了一个**Self-MM**框架，通过自监督多任务学习获取特定模态的表示：
- 利用**自监督标签生成模块**自动生成单模态标签，无需额外的人工标注。
- 通过**动态权重调整策略**平衡模态一致性和特定差异性。
- 在MOSI、MOSEI和SIMS等基准数据集上优于当前最先进方法。

## 方法
**Self-MM框架**主要包括以下模块：
1. **多模态任务**：通过预训练的BERT和单向LSTM模型处理文本、音频和视觉数据，生成融合的多模态表示。
2. **单模态子任务**：通过自动生成的标签训练单模态任务，捕捉特定模态的特征。
3. **单模态标签生成模块（ULGM）**：根据多模态数据动态计算标签，并通过动量更新策略在训练中保持稳定性。
4. **优化策略**：通过多任务损失函数结合权重调整机制，优先处理模态标签差异较大的样本。

## 实验结果
在三个广泛使用的多模态情感分析数据集上验证了Self-MM的性能：
- **MOSI**：在多模态情感回归任务中实现了最先进的相关性和分类准确率。
- **MOSEI**：在大规模数据集上展现了出色的鲁棒性，超过了MAG-BERT和MISA等基线模型。
- **SIMS**：在情感预测上与人工标注的标签表现相当，验证了自动生成标签的可靠性。
完整的实现代码和预处理数据集已开放，点击下方链接访问：
[**GitHub 仓库**](https://github.com/thuiar/Self-MM)

### 主要实验结果
| 数据集 | MAE ↓ | 相关性（Corr）↑ | 准确率（Acc-2）↑ | F1分数 ↑ |
|--------|--------|----------------|-----------------|----------|
| MOSI   | 0.713  | 0.798          | 84.00           | 85.98    |
| MOSEI  | 0.530  | 0.765          | 82.81           | 85.17    |
| SIMS   | 0.419  | 0.616          | 80.74           | 80.78    |

Self-MM显著提高了模态特定和模态一致表示的学习效果，自动生成的单模态标签为人工标注提供了一种经济高效的替代方案。

---

# Speak From Heart: 基于情感引导的LLM多模态对话生成方法

![image](https://github.com/user-attachments/assets/4582a223-9e19-4a0b-874c-b5732a4d2a9f)

## 简介
本论文提出了 **ELMD**（Emotion-Guided LLM-Based Multimodal Dialogue），一种基于大语言模型（LLM）的情感引导多模态对话框架。ELMD通过融合文本和视觉线索，显著提升了对话系统的情感表达能力，能够生成更具情感共鸣且贴近人类的自然对话内容，解决了传统情感对话生成中的诸多局限性。

## 方法
ELMD框架主要由以下三个核心模块组成：
1. **情感检索模块（ERM）：** 使用对比学习获取细粒度的情感表示，为生成示例提供语义和情感相关内容。
2. **情感预测模块（REP）：** 将视觉信息中的情感线索与生成的响应关联起来，增强情感理解能力。
3. **情感增强响应生成模块（EERG）：** 融合多模态输入，生成流畅且富有情感的对话内容。

通过两阶段训练策略，ELMD能够捕捉细腻情感并实现多模态信息的高效协同。

## 实验结果
在两个真实场景的中文多模态数据集（M3ED 和 MMED）上进行的实验表明，ELMD在 BLEU、METEOR、ROUGE 等评价指标上显著优于现有基线模型（如 VisualGLM 和 DialoGPT），模型展现了在情感表达和语境理解方面的强大性能。

---
